{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import cv2\n",
    "import pdb\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_batchsize = 32\n",
    "eval_test_batchsize = 16\n",
    "epochs = 16\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the device checking & setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By defalt, set device to the CPU\n",
    "deviceFlag = torch.device('cpu')\n",
    "\n",
    "# Defalut is CPU, but as long as GPU is avaliable, then use GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f'Found {torch.cuda.device_count()} GPUs.')\n",
    "    deviceFlag = torch.device('cuda:0')\n",
    "\n",
    "print(f'Now the deivce is set to {deviceFlag}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @ The Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, validateloader, ValCriterion):\n",
    "    \n",
    "    val_loss_running = 0\n",
    "    acc = 0\n",
    "    \n",
    "    # a dataloader object is a generator of batches, each batch contain image & label separately\n",
    "    for images, labels in iter(validateloader):\n",
    "        \n",
    "        # Send the data onto choosen device\n",
    "        images = images.to(deviceFlag)\n",
    "        labels = labels.to(deviceFlag)\n",
    "        \n",
    "        output = model.forward(images)\n",
    "        val_loss_running += ValCriterion(output, labels).item() # .item() to get a scalar in Torch.tensor out\n",
    "        \n",
    "        output = torch.exp(output) # as in the model we use the .LogSoftmax() output layer\n",
    "        \n",
    "        equals = (labels.data == output.max(dim = 1)[1])\n",
    "        acc += equals.float().mean().item() # .flaot() is to transfer the tensor.cuda.float type onto cpu mode\n",
    "        \n",
    "    return val_loss_running / len(validateloader), acc / len(validateloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @ The Training (&Validating) Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(model, traindataloader, validateloader, TrCriterion, optimizer, epochs, deviceFlag_train):\n",
    "    \n",
    "    itrs = 0\n",
    "    eval_itrs = 40\n",
    "    \n",
    "    # first setting the device used for training\n",
    "    model.to(deviceFlag_train)\n",
    "    \n",
    "    print(f'The training batchsize is {tr_batchsize}.')\n",
    "    \n",
    "    # set the timer\n",
    "    since = time.time()\n",
    "\n",
    "    # ! THE EPOCH LOOP !\n",
    "    for e in range(epochs):        \n",
    "        itrs = 0\n",
    "        \n",
    "        # Set the model to the Train mode\n",
    "        # Tell the model to activate its Training behavior (turn-on the dropout & BN behaviors)\n",
    "        model.train()\n",
    "        \n",
    "        # re-initialize the running_loss to start every epoch\n",
    "        training_loss_running = 0\n",
    "        \n",
    "        #  ! THE BATCH LOOP !\n",
    "        for inputs, labels in iter(traindataloader):            \n",
    "            itrs += 1\n",
    "            # .to() method return a copy of the tensors on the targeted device\n",
    "            inputs = inputs.to(deviceFlag_train)\n",
    "            labels = labels.to(deviceFlag_train)\n",
    "            \n",
    "            # Clean the stored grads computed in the last iteration\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward Pass\n",
    "            # As model has been shipped to the targeted device, so the output is on that device too\n",
    "            outputs = model.forward(inputs)\n",
    "            \n",
    "            # Compute Loss\n",
    "            train_loss = TrCriterion(outputs, labels)\n",
    "            \n",
    "            # BackProp to compute the grads (stored in each tensor.grad() attributes) along the way\n",
    "            train_loss.backward()\n",
    "            \n",
    "            # Optimizer/Update params\n",
    "            optimizer.step()\n",
    "            \n",
    "            training_loss_running += train_loss.item() #numeric ops, take the scalar out of the tensor by calling .item()\n",
    "            \n",
    "            # ----------- Perform Validation (Evaluation) Every eval_itrs iterations ---------- #\n",
    "            if itrs % eval_itrs == 0:\n",
    "                \n",
    "                # Set the model to the Eval mode\n",
    "                model.eval()\n",
    "                \n",
    "                # Turn-off gradient for validation to save memory & computation\n",
    "                with torch.no_grad():\n",
    "                    validation_loss, val_acc = validation(model, validateloader, TrCriterion)\n",
    "                \n",
    "                display = f'Epoch: {e + 1}/{epochs}, itrs: {itrs}, '\n",
    "                display += f'Train_loss: {round(training_loss_running / eval_itrs, 4)}, '\n",
    "                display += f'Valid_loss: {round(validation_loss, 4)}, '\n",
    "                display += f'Valid_Acc: {round(val_acc, 4)}'\n",
    "                print(display)\n",
    "                \n",
    "                training_loss_running = 0\n",
    "                model.train()\n",
    "                \n",
    "        end = time.time()\n",
    "        elapsed = end - since\n",
    "        print(f'Epoch {e + 1} takes {round(elapsed, 4)} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @ The TEST function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_acc(model, testloader, deviceFlag_test):\n",
    "\n",
    "    # for testing, it is actually do validation on the test set\n",
    "    model.eval()\n",
    "\n",
    "    model.to(deviceFlag_test)\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    # In .eval() mode, set the context manager to turn-off grads\n",
    "    with torch.no_grad():\n",
    "        acc = 0\n",
    "\n",
    "        # iter() gives images and labels in batches\n",
    "        for inputs, labels in iter(test_loader):\n",
    "            \n",
    "            inputs = inputs.to(deviceFlag_test)\n",
    "            labels = labels.to(deviceFlag_test)\n",
    "\n",
    "            # Do a forward pass\n",
    "            output = model.forward(inputs)\n",
    "            # convert the log likelihood to scalar\n",
    "            prob = torch.exp(output)\n",
    "\n",
    "            equals = (labels.data == prob.max(dim = 1)[1])\n",
    "\n",
    "            acc += equals.type(torch.FloatTensor).mean().item()\n",
    "\n",
    "        end = time.time()\n",
    "        elapsed = end - since\n",
    "\n",
    "        print(f'Test_acc: {round(acc, 4)}, tiem_spent: {round(elapsed, 2)} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @ The Chkpt saving function\n",
    "A chkpt is a dictionary.\n",
    "save as much info you need for loading and testing.\n",
    "If want to load then keep on trianing with where it left off, remmeber to save the optimizer.state_dict too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, trainingdataset, saved_pth):\n",
    "    # set a new attr to the model object, which holds the class_to_idx conversion\n",
    "    model.class_to_idx = trainingdataset.class_to_idx\n",
    "    \n",
    "    # Chkpt is a dictionary, can be modified to hold anything you need in the furture\n",
    "    chkpt = {\n",
    "    'arch': 'vgg19',\n",
    "    'class_to_idx': model.class_to_idx,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "   # 'optimizer_state_dict': optimizer.state_dict()\n",
    "    }\n",
    "    \n",
    "    # Save with torch.save\n",
    "    torch.save(chkpt, saved_pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @ The Chkpt Loading function\n",
    "Need to re-initialzie (re-build in the same way as the model is built before training it) a new model with the same arch as stored in the chkpt, then load the .stat_dict() params into this new arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(chkpt_path):\n",
    "    \n",
    "    chkpt = torch.load(chkpt_path)\n",
    "    \n",
    "    # After loading, the elements stored in the chkpt can be accesses as in a dict with key & value\n",
    "    if chkpt['arch'] == 'vgg19':\n",
    "        # Re-initial a new network arch\n",
    "        model = models.vgg19(pretrained = True)\n",
    "        \n",
    "        # Turn-off the .requires_grad attributes for all params in the feature extraction head\n",
    "        for params in model.parameters():\n",
    "            params.requires_grad = False\n",
    "    \n",
    "    else:\n",
    "        print('------- Wrong Network Architecture is being used----------')\n",
    "    \n",
    "    model.class_to_idx = chkpt['class_to_idx']\n",
    "    \n",
    "    # Re-inital a new empty classisifer\n",
    "    \n",
    "    classifier = nn.Sequential(OrderedDict([\n",
    "        ('fc1', nn.Linear(25088, 4096)),\n",
    "        ('relu', nn.ReLU()),\n",
    "        ('drop', nn.Dropout(p = 0.5)),\n",
    "        ('fc2', nn.Linear(4096, 102)),\n",
    "        ('output', nn.LogSoftmax(dim = 1))\n",
    "    ]))\n",
    "    \n",
    "    # Attach the classifer head\n",
    "    model.classifier = classifier\n",
    "    \n",
    "    # Load the params stored in the chkpt into the newly constructed empty model\n",
    "    # model.load_state_dict() is a built-in method of the models object\n",
    "    model.load_state_dict(chkpt['model_state_dict'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @ Image PreProcessing Function\n",
    "Using the PIL Image module\n",
    "1. Resize image while keeping aspect ratio: thumbnail() or resize();\n",
    "2. Crop the image into the required size;\n",
    "3. Normalizing (<font color = 'red'>convert PIL image object to np.array: np_image = np.array(PIL_image))</font>\n",
    "4. Switch the array axes from <font color = 'red'>[H, W, C] (np.array & PIL image)  to  [C, H, W]  (PT tensor) </font> using ndarray.transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(img_pth):\n",
    "    '''\n",
    "    Input a PIL image, output a numpy array with axes transposed to [Ch, H, W]\n",
    "    '''\n",
    "    pil_image = Image.open(img_pth)\n",
    "    \n",
    "    # -------- Resize with Aspect Ratio maintained--------- #\n",
    "    # First fixing the short axes\n",
    "    if pil_image.size[0] > pil_image.size[1]:\n",
    "        pil_image.thumbnail((10000000, 256))\n",
    "    else:\n",
    "        pil_image.thumbnail((256, 100000000))\n",
    "    \n",
    "    # ---------Crop----------- #\n",
    "    left_margin = (pil_image.width - 224) / 2\n",
    "    bottom_margin = (pil_image.height - 224) / 2\n",
    "    right_margin = left_margin + 224\n",
    "    top_margin = bottom_margin + 224\n",
    "    \n",
    "    pil_image = pil_image.crop((left_margin, bottom_margin, right_margin, top_margin))\n",
    "    \n",
    "    # --------- Convert to np then Normalize ----------- #\n",
    "    np_image = np.array(pil_image) / 255\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    np_image = (np_image -mean) / std\n",
    "    \n",
    "    # --------- Transpose to fit PyTorch Axes ----------#\n",
    "    np_image = np_image.transpose([2, 0, 1])\n",
    "    \n",
    "    return np_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @ Image Dispalying Function with Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(pt_image, ax = None, title = None):\n",
    "    '''\n",
    "    Takes in a PyTorch-compatible image with [Ch, H, W],\n",
    "    Convert it back to [H, W, Ch], \n",
    "    Undo the preprocessing,\n",
    "    then display it on a grid\n",
    "    '''\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    # --------- Transpose ----------- #\n",
    "    plt_image = pt_image.transpose((1, 2, 0))\n",
    "    \n",
    "    # --------- Undo the preprocessing --------- #\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    plt_image = plt_image * std + mean\n",
    "    \n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "        \n",
    "    # Image need to be clipped between 0 and 1 or it looks noisy\n",
    "    plt_image = np.clip(plt_image, 0, 1)\n",
    "    \n",
    "    # this imshow is a function defined in the plt module\n",
    "    ax.imshow(plt_image)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @ Prediction Function\n",
    "1. input a path to a img, preprocess it with image_preprocessing()\n",
    "2. forward pass on a model\n",
    "3. use tensor.topk(k) to return the highest k probs and the correspodniung class idx\n",
    "4. convert the idx to class names using the name_to_idx conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_pth, model, trainingdataset, topk):\n",
    "    '''\n",
    "    1. input a single img;\n",
    "    2. forward pass on a model;\n",
    "    3. use tensor.topk(k) to return the highest k probs and the correspodniung class idx;\n",
    "    4. convert the idx to class names using the name_to_idx conversion.\n",
    "    '''\n",
    "    np_img = image_preprocessing(img_pth)\n",
    "    \n",
    "    # Convert np_img to PT tensor and send to GPU\n",
    "    pt_img = torch.from_numpy(np_img).type(torch.cuda.FloatTensor)\n",
    "    \n",
    "    # Unsqueeze to get shape of tensor from [Ch, H, W] to [Batch, Ch, H, W]\n",
    "    pt_img = pt_img.unsqueeze(0)\n",
    "    \n",
    "    # Run the model to predict\n",
    "    output = model.forward(pt_img)\n",
    "    \n",
    "    probs = torch.exp(output)\n",
    "    \n",
    "    # Pick out the topk from all classes \n",
    "    top_probs, top_indices = probs.topk(topk)\n",
    "    \n",
    "    # Convert to list on CPU without grads\n",
    "    top_probs = top_probs.detach().type(torch.FloatTensor).numpy().tolist()[0]\n",
    "    top_indices = top_indices.detach().type(torch.FloatTensor).numpy().tolist()[0]\n",
    "    \n",
    "    # Invert the class_to_idx dict to a idx_to_class dict\n",
    "    idx_to_class = {value: key for key, value in trainingdataset.class_to_idx.items()}\n",
    "    \n",
    "    top_classname = {idx_to_class[index] for index in top_indices}\n",
    "    \n",
    "    return top_probs, top_classname    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA LOADING\n",
    "here the 102 flowers dataset is pre-downloaded at local, so no need to call the torchvision.datasets.CIFAR10() etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'flowers'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/valid'\n",
    "test_dir = data_dir + '/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(), # For GPU purpose\n",
    "    # As we are going to do transfer learning with a ImageNet pretrained VGG\n",
    "    # so here we normalize the dataset being used here with the ImageNet stats\n",
    "    # for better transfer learning performance\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], # RGB mean & std estied on ImageNet\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "validation_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], # RGB mean & std estied on ImageNet\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "testing_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], # RGB mean & std estied on ImageNet\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the datasets with torchvision.datasets.ImageFolder object\n",
    "training_imagefolder = datasets.ImageFolder(train_dir, transform = training_transforms)\n",
    "validation_imagefolder = datasets.ImageFolder(valid_dir, transform = validation_transforms)\n",
    "testing_imagefolder = datasets.ImageFolder(test_dir, transform = testing_transforms)\n",
    "\n",
    "# Define the torch.utils.data.DataLoader() object with the ImageFolder object\n",
    "# Dataloader is a generator to read from ImageFolder and generate them into batch-by-batch\n",
    "# Only shuffle during trianing, validation and testing no shuffles\n",
    "# the batchsize for training and tesitng no need to be the same\n",
    "train_loader = torch.utils.data.DataLoader(training_imagefolder, batch_size = tr_batchsize, shuffle = True)\n",
    "validate_loader = torch.utils.data.DataLoader(validation_imagefolder, batch_size = eval_test_batchsize)\n",
    "test_loader = torch.utils.data.DataLoader(testing_imagefolder, batch_size = eval_test_batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes name to class index number Mapping with JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('flower_to_name.json', 'r') as f:\n",
    "    flower_to_name = json.load(f)\n",
    "\n",
    "#print(len(flower_to_name))\n",
    "#print(flower_to_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET pretrained VGG with a new classifier head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg19(pretrained = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze the params in the feature head, by setting .requries.grad to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in model.parameters():\n",
    "    params.requries_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a customed classifier head with 102 output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewClassifier = nn.Sequential(OrderedDict([\n",
    "    ('fc1', nn.Linear(25088, 4096)),\n",
    "    ('relu', nn.ReLU()),\n",
    "    ('drop', nn.Dropout(p = 0.5)),\n",
    "    ('fc2', nn.Linear(4096, 102)),\n",
    "    ('output', nn.LogSoftmax(dim = 1))\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attach the new classifier head onto the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = NewClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the loss func and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Log Likelihood Loss\n",
    "criterion = nn.NLLLoss()\n",
    "# in the optimzier, define the range of params to be updated\n",
    "# Here we only train the params in the model.classifier part (model object has the classifier attribute)\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Eval the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval(model, train_loader, validate_loader, criterion, optimizer, epochs, deviceFlag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc(model, test_loader, deviceFlag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Chkpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_checkpoint(model, optimizer, training_imagefolder, 'chkpt.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#load_checkpoint('chkpt.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE Stage\n",
    "(1) IMAGE PREPROCESSING: Pre-proccess 1 image according to the format that recquried by the Network <br>\n",
    "(2) Display the preprocessed image <br>\n",
    "(3) Perform prediction on this image (by forward-pass it through the trained or loaded network) <br>\n",
    "(4) Sanity Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image preprocessing then display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image = image_preprocessing('flowers/test/1/image_06743.jpg')\n",
    "imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probs, classes = predict('flowers/test/15/image_06369.jpg', model, training_imagefolder, 5)   \n",
    "print(probs)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Checking Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an image along with the top 5 classes\n",
    "\n",
    "# Plot flower input image\n",
    "plt.figure(figsize = (6,10))\n",
    "plot_1 = plt.subplot(2,1,1)\n",
    "\n",
    "image = image_preprocessing('flowers/test/15/image_06369.jpg')\n",
    "\n",
    "flower_title = flower_to_name['15']\n",
    "\n",
    "imshow(image, plot_1, title=flower_title);\n",
    "\n",
    "# Convert from the class integer encoding to actual flower names\n",
    "flower_names = [flower_to_name[i] for i in classes]\n",
    "\n",
    "# Plot the probabilities for the top 5 classes as a bar graph\n",
    "plt.subplot(2,1,2)\n",
    "\n",
    "sb.barplot(x=probs, y=flower_names, color=sb.color_palette()[0]);\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
